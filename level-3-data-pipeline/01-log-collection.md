# 로그 수집 구조

## 1. 한 줄 요약

모바일 앱에서 발생하는 이벤트 로그를 **안정적으로, 순서대로, 유실 없이** 백엔드 시스템으로 전달하는 아키텍처입니다.

---

## 2. 쉽게 설명

### 모바일 개발자 관점의 비유

**택배 물류 시스템**을 생각해보세요.

- **모바일 앱** = 전국의 편의점 택배 접수처
- **로그 수집 서버** = 지역 물류 허브
- **Kafka/Kinesis** = 대형 물류 센터 (분류 및 버퍼링)
- **S3** = 최종 창고

편의점에서 택배를 보내면, 지역 허브에 모였다가, 대형 물류 센터에서 분류되어, 최종 창고로 갑니다. 로그도 마찬가지입니다.

### 왜 직접 DB에 저장하지 않는가?

```
❌ 잘못된 방식 (동기적 직접 저장)
App ──HTTP──▶ API Server ──INSERT──▶ Database
              │
              └── 문제: DB가 느리면 앱도 느려짐
                        DB가 죽으면 로그 유실
```

```
✅ 올바른 방식 (비동기 버퍼링)
App ──HTTP──▶ Collector ──▶ Kafka ──▶ Consumer ──▶ Storage
              │                │
              └── Fire & Forget  └── 버퍼링 & 재시도
```

---

## 3. 구조 다이어그램

### 전체 아키텍처

```
┌─────────────────────────────────────────────────────────────────────────┐
│                        LOG COLLECTION ARCHITECTURE                       │
└─────────────────────────────────────────────────────────────────────────┘

 📱 iOS/Android App
        │
        │ HTTP/HTTPS (JSON)
        ▼
┌──────────────────┐
│   Load Balancer  │  ◀── 트래픽 분산
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│  Log Collector   │  ◀── Fluentd, Logstash, 커스텀 서버
│   (Stateless)    │      - 포맷 정규화
└────────┬─────────┘      - 기본 검증
         │
         ▼
┌──────────────────┐
│  Message Queue   │  ◀── Kafka, Kinesis, Pub/Sub
│   (Buffer)       │      - 순서 보장
│                  │      - 재처리 가능
│  ┌────┬────┬────┐│      - 확장성
│  │ P0 │ P1 │ P2 ││  ◀── 파티션별 병렬 처리
│  └────┴────┴────┘│
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│    Consumer      │  ◀── Spark Streaming, Flink
│   (Processing)   │      - 실시간 처리
└────────┬─────────┘      - 배치 처리
         │
         ▼
┌──────────────────┐
│     Storage      │  ◀── S3, HDFS
│   (Data Lake)    │
└──────────────────┘
```

### Kafka 토픽 설계 예시

```
┌─────────────────────────────────────────────────────────┐
│                    KAFKA TOPICS                          │
├─────────────────────────────────────────────────────────┤
│                                                          │
│  logs.app.click          ──▶ 클릭 이벤트                 │
│  logs.app.screen_view    ──▶ 화면 조회                   │
│  logs.app.purchase       ──▶ 구매 이벤트 (중요!)         │
│  logs.app.error          ──▶ 에러 로그                   │
│                                                          │
│  각 토픽 구성:                                            │
│  ┌────────────────────────────────────────────────────┐ │
│  │ Partitions: 12개 (Consumer 병렬성)                  │ │
│  │ Replication: 3개 (고가용성)                         │ │
│  │ Retention: 7일 (재처리 가능 기간)                   │ │
│  └────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────┘
```

---

## 4. 실무 적용 예시

### 4.1 모바일 앱의 로그 전송 구현

```swift
// iOS - 로그 배치 전송 예시
class LogCollector {
    private var buffer: [LogEvent] = []
    private let maxBufferSize = 100
    private let flushInterval: TimeInterval = 30  // 30초

    func track(event: LogEvent) {
        buffer.append(event)

        if buffer.count >= maxBufferSize {
            flush()
        }
    }

    func flush() {
        guard !buffer.isEmpty else { return }

        let payload = LogPayload(
            events: buffer,
            deviceId: DeviceInfo.id,
            timestamp: Date()
        )

        // 비동기 전송, 실패 시 로컬 저장
        APIClient.sendLogs(payload) { [weak self] result in
            switch result {
            case .success:
                self?.buffer.removeAll()
            case .failure:
                self?.persistToLocal(payload)
            }
        }
    }
}
```

### 4.2 로그 포맷 표준화

```json
{
    "event_name": "button_click",
    "event_timestamp": "2024-01-15T10:30:00Z",
    "event_params": {
        "button_id": "purchase_btn",
        "screen_name": "product_detail",
        "product_id": "SKU12345"
    },
    "user_context": {
        "user_id": "u_abc123",
        "session_id": "s_xyz789",
        "is_logged_in": true
    },
    "device_context": {
        "device_id": "d_qwe456",
        "platform": "iOS",
        "app_version": "2.3.1",
        "os_version": "17.2"
    },
    "collection_metadata": {
        "sdk_version": "1.2.0",
        "sent_at": "2024-01-15T10:30:05Z"
    }
}
```

### 4.3 Kafka Consumer 설정 예시

```yaml
# Consumer Group 설정
consumer:
  group.id: log-processor-group
  auto.offset.reset: earliest  # 처음부터 읽기
  enable.auto.commit: false    # 수동 커밋 (정확히 한 번 처리)
  max.poll.records: 500        # 한 번에 가져올 레코드 수

# 파티션 할당
# user_id를 키로 사용 → 같은 유저의 이벤트는 같은 파티션
partition.key: user_id
```

---

## 5. 장단점

### Kafka vs Kinesis 비교

| 항목 | Kafka | Kinesis |
|------|-------|---------|
| **관리** | 직접 운영 (or MSK) | 완전 관리형 |
| **비용** | 초기 비용 높음 | 사용량 기반 |
| **처리량** | 매우 높음 | 높음 (샤드당 제한) |
| **지연시간** | 매우 낮음 (ms) | 낮음 (수십 ms) |
| **보존기간** | 무제한 가능 | 최대 365일 |
| **생태계** | 풍부함 | AWS 서비스 연동 |
| **러닝커브** | 높음 | 낮음 |

### 로그 수집 아키텍처의 장점

- **확장성**: 트래픽 증가에 유연하게 대응
- **내구성**: 메시지 유실 방지
- **유연성**: 다양한 Consumer 추가 가능
- **재처리**: 문제 발생 시 과거 데이터 재처리

### 로그 수집 아키텍처의 단점

- **복잡성**: 관리할 컴포넌트 증가
- **지연**: 실시간이 아닌 준실시간 (수 초~수 분)
- **비용**: 인프라 비용 증가
- **디버깅**: 문제 발생 시 추적이 어려움

---

## 6. 내 생각

```
(학습 후 자신의 생각을 정리해보세요)

- 현재 우리 앱의 로그 수집 방식은?

- 개선이 필요한 부분은?

- Kafka와 Kinesis 중 우리 상황에 맞는 것은?

```

---

## 7. 추가 질문

### 기초 레벨
1. 로그를 배치로 모아서 보내는 이유는 무엇인가요?

> **답변**: 로그를 배치로 모아서 보내는 이유는 크게 세 가지입니다. 첫째, **네트워크 효율성**입니다. 각 HTTP 요청에는 헤더, TCP 핸드셰이크 등의 오버헤드가 있는데, 100개의 이벤트를 개별 전송하면 100번의 오버헤드가 발생하지만 배치로 묶으면 1번으로 줄어듭니다. 둘째, **배터리 절약**입니다. 모바일 기기에서 네트워크 호출은 라디오 칩을 활성화시키는데, 빈번한 호출은 배터리를 빠르게 소모합니다. 셋째, **서버 부하 감소**입니다. 동시접속자 10만 명이 매 초 이벤트를 보내면 초당 10만 요청이지만, 30초마다 배치로 모으면 초당 약 3,300 요청으로 줄어듭니다. 실무에서는 보통 100개 이벤트 또는 30초 간격 중 먼저 도달하는 조건으로 배치를 전송합니다.

2. 모바일 앱이 오프라인일 때 로그는 어떻게 처리해야 할까요?

> **답변**: 오프라인 상태에서는 로그를 **로컬 저장소(SQLite, CoreData, Room)에 임시 저장**해야 합니다. 구체적인 구현 방법은 다음과 같습니다. 첫째, 네트워크 상태를 모니터링하여 오프라인 감지 시 로컬 DB에 이벤트를 저장합니다. 둘째, 저장할 때 타임스탬프, 이벤트 데이터, 재시도 횟수를 함께 기록합니다. 셋째, 네트워크가 복구되면 저장된 로그를 시간 순서대로 서버에 전송합니다. 넷째, 로컬 저장소 용량 제한(예: 최대 10MB 또는 1만 개)을 설정하고, 초과 시 오래된 이벤트부터 삭제합니다. Firebase Analytics나 Amplitude 같은 SDK들도 내부적으로 이 방식을 사용합니다. 중요한 점은 오프라인 동안 쌓인 로그가 온라인 복귀 시 한꺼번에 전송되지 않도록 **점진적 전송(throttling)**을 적용하는 것입니다.

3. 로그 전송 실패 시 재시도 정책은 어떻게 설계해야 할까요?

> **답변**: 재시도 정책은 **지수 백오프(Exponential Backoff)**를 기본으로 설계합니다. 예를 들어 첫 번째 실패 후 1초, 두 번째 실패 후 2초, 세 번째 실패 후 4초, 네 번째 실패 후 8초 대기하는 방식입니다. 추가로 **Jitter(무작위 지연)**를 적용하여 여러 클라이언트가 동시에 재시도하는 "썬더링 허드(Thundering Herd)" 문제를 방지합니다. 재시도 횟수는 보통 3~5회로 제한하고, 최대 재시도 후에도 실패하면 로컬에 저장하여 나중에 다시 시도합니다. HTTP 상태 코드에 따라 재시도 여부를 다르게 처리하는 것도 중요한데, 5xx(서버 에러)와 429(Rate Limit)는 재시도하고, 4xx(클라이언트 에러 중 400, 401, 403)는 재시도하지 않습니다. iOS에서는 URLSession의 waitsForConnectivity, Android에서는 WorkManager의 BackoffPolicy를 활용할 수 있습니다.

### 심화 레벨
4. Kafka 파티션 수는 어떻게 결정하나요?

> **답변**: Kafka 파티션 수는 **처리량 요구사항과 Consumer 병렬성**을 고려하여 결정합니다. 기본 공식은 `파티션 수 = max(처리량 / 파티션당 처리량, Consumer 수)`입니다. 예를 들어, 초당 10만 메시지를 처리하고 파티션당 초당 1만 메시지를 처리할 수 있다면 최소 10개 파티션이 필요합니다. Consumer Group의 Consumer 수가 12개라면 12개 파티션이 적합합니다. 실무에서 고려할 점은: (1) 파티션 수는 나중에 늘릴 수 있지만 줄이기 어렵습니다, (2) 파티션이 너무 많으면 메타데이터 오버헤드가 증가합니다, (3) 보통 토픽당 12~24개로 시작하여 필요에 따라 확장합니다. 파티셔닝 키(보통 user_id)를 사용하면 같은 키의 메시지가 같은 파티션으로 가서 순서가 보장됩니다.

5. Exactly-once 전송을 보장하려면 어떻게 해야 하나요?

> **답변**: Exactly-once(정확히 한 번) 전송은 데이터 파이프라인에서 가장 어려운 문제 중 하나입니다. 완벽한 exactly-once는 어렵지만, **멱등성(Idempotency)**과 **트랜잭션**을 조합하여 구현합니다. 첫째, **Producer 멱등성**: Kafka Producer에서 `enable.idempotence=true`를 설정하면 네트워크 재시도로 인한 중복 메시지를 방지합니다. 둘째, **Consumer 멱등성**: Consumer에서 메시지 처리 시 고유 ID(event_id)를 기반으로 중복 처리를 방지합니다. Redis나 DB에 처리된 ID를 저장하고 이미 처리된 메시지는 스킵합니다. 셋째, **트랜잭션**: Kafka Streams에서 `processing.guarantee=exactly_once_v2`를 사용하면 읽기-처리-쓰기 전체를 원자적으로 처리합니다. 모바일 앱에서는 **클라이언트 측에서 UUID 기반 event_id를 생성**하여 서버에서 중복 제거하는 방식이 일반적입니다.

6. 로그 스키마가 변경될 때 하위 호환성은 어떻게 유지하나요?

> **답변**: 스키마 변경 시 하위 호환성을 유지하는 핵심 원칙은 **"추가는 허용, 삭제와 타입 변경은 금지"**입니다. 구체적인 방법은 다음과 같습니다. 첫째, **새 필드 추가** 시 기본값을 설정하거나 optional로 정의합니다 (예: Avro, Protocol Buffers 사용). 둘째, **필드 삭제** 대신 deprecated 표시 후 Consumer가 모두 업데이트된 후 제거합니다. 셋째, **타입 변경**이 필요하면 새 필드를 추가하고 점진적으로 마이그레이션합니다 (예: `age` → `age_v2`). 넷째, **Schema Registry(Confluent Schema Registry)를 사용하여 스키마 버전을 관리**하고, 호환성 검사를 자동화합니다. 앱 버전별로 스키마가 다를 수 있으므로, `schema_version` 필드를 로그에 포함하여 Consumer가 버전별로 다르게 처리할 수 있게 합니다. 실무에서는 앱 업데이트 주기가 느리므로 최소 6개월~1년간 이전 스키마를 지원해야 합니다.

### 실무 시나리오
7. DAU 100만 앱에서 예상되는 일일 로그 양은 얼마인가요?

> **답변**: DAU 100만 앱의 일일 로그 양을 계산해보겠습니다. **가정**: 사용자당 세션 수 2회, 세션당 이벤트 25개, 이벤트당 평균 크기 1KB. **계산**: 100만 × 2 × 25 × 1KB = **50GB/일 (5천만 이벤트)**. 실제로는 앱 종류에 따라 크게 다릅니다. 게임 앱은 이벤트가 많아 100~200GB, 뉴스 앱은 20~30GB 정도입니다. 월간으로 환산하면 1.5TB, 연간 18TB입니다. 여기에 로그의 압축률(보통 70~80%)을 적용하면 저장 용량은 줄어들지만, 처리해야 할 원시 데이터 양은 동일합니다. 이 규모에서는 반드시 Kafka 같은 메시지 큐와 Spark 같은 분산 처리 시스템이 필요하며, S3에 Parquet 형식으로 저장하면 비용을 크게 절감할 수 있습니다.

8. 블랙프라이데이 같은 트래픽 스파이크에 어떻게 대비하나요?

> **답변**: 트래픽 스파이크 대비는 **사전 준비, 아키텍처 설계, 모니터링** 세 가지 측면에서 접근합니다. **사전 준비**: (1) 과거 데이터를 분석하여 예상 트래픽을 산정합니다(보통 평소의 3~10배), (2) 부하 테스트로 시스템 한계를 파악합니다, (3) Auto Scaling 규칙을 미리 설정합니다. **아키텍처 설계**: (1) Kafka/Kinesis 파티션을 충분히 확보합니다, (2) Log Collector는 Stateless로 설계하여 수평 확장이 쉽게 합니다, (3) 배압(Backpressure) 메커니즘으로 Consumer가 처리 가능한 속도로 조절합니다, (4) 클라이언트 측에서 샘플링(예: 50%만 전송)을 적용할 수 있는 기능을 미리 구현합니다. **모니터링**: (1) Kafka lag, Consumer 처리량, 에러율을 실시간 모니터링합니다, (2) 임계치 초과 시 알림을 설정합니다, (3) 비상 시 덜 중요한 이벤트를 드롭하는 정책을 준비합니다.

9. 민감한 개인정보가 로그에 포함되면 어떻게 처리해야 하나요?

> **답변**: 개인정보 처리는 **수집 단계 최소화, 전송 단계 암호화, 저장 단계 익명화/삭제**의 원칙을 따릅니다. **수집 단계**: (1) 애초에 불필요한 개인정보는 수집하지 않습니다 (데이터 최소화 원칙), (2) 클라이언트에서 이메일, 전화번호 등은 해싱 후 전송합니다, (3) 로깅 SDK에 PII(개인식별정보) 필터링 규칙을 적용합니다. **전송 단계**: HTTPS(TLS)를 필수로 사용하고, 민감한 데이터는 추가 암호화(AES 등)를 적용합니다. **저장 단계**: (1) S3에 저장 시 서버 측 암호화(SSE-S3, SSE-KMS)를 적용합니다, (2) ETL 과정에서 비식별화(이름 → 해시, IP → 대역대만 저장)를 수행합니다, (3) GDPR/개인정보보호법에 따라 삭제 요청 처리 프로세스를 구축합니다. 특히 **Right to be Forgotten** 요청이 들어오면 모든 데이터 저장소에서 해당 사용자의 데이터를 삭제할 수 있는 파이프라인이 필요합니다.

---

## 8. 모바일 앱에서 Kafka까지: 데이터 흐름 상세

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                    모바일 앱 로그의 여정 (End-to-End)                              │
└─────────────────────────────────────────────────────────────────────────────────┘

  STEP 1: 이벤트 발생 (앱 내부)
  ─────────────────────────────
  📱 사용자가 "구매하기" 버튼 클릭
        │
        ▼
  ┌─────────────────────────────────┐
  │  LogCollector.track(            │
  │    event: "button_click",       │
  │    params: {                    │
  │      button_id: "purchase_btn", │
  │      product_id: "SKU123"       │
  │    }                            │
  │  )                              │
  └─────────────────────────────────┘
        │
        ▼
  STEP 2: 로컬 버퍼에 저장
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  In-Memory Buffer               │
  │  ┌───────────────────────────┐  │
  │  │ [Event 1] [Event 2] [...] │  │  ← 100개 또는 30초까지 대기
  │  └───────────────────────────┘  │
  └─────────────────────────────────┘
        │
        │ 조건 충족 시 (100개 도달 or 30초 경과)
        ▼
  STEP 3: HTTP 배치 전송
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  POST /v1/events                │
  │  Content-Type: application/json │
  │  Authorization: Bearer <token>  │
  │  X-Device-ID: d_qwe456         │
  │                                 │
  │  Body: {                        │
  │    "events": [...100개...],     │
  │    "sent_at": "2024-01-15T..."  │
  │  }                              │
  └─────────────────────────────────┘
        │
        │ HTTPS (TLS 1.3)
        ▼
  STEP 4: API Gateway / Load Balancer
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  • Rate Limiting 검사           │
  │  • 인증 토큰 검증                │
  │  • 트래픽 분산                   │
  └─────────────────────────────────┘
        │
        ▼
  STEP 5: Log Collector Server (Stateless)
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  • JSON 파싱 및 검증             │
  │  • 스키마 유효성 검사            │
  │  • 타임스탬프 정규화             │
  │  • 서버 측 메타데이터 추가       │
  │    (received_at, server_ip)     │
  └─────────────────────────────────┘
        │
        │ 즉시 응답 (202 Accepted) - Fire & Forget
        ▼
  STEP 6: Kafka Producer
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  • 토픽 결정 (이벤트 유형별)     │
  │  • 파티션 키 설정 (user_id)     │
  │  • 직렬화 (Avro/JSON)          │
  │  • 배치로 묶어서 전송           │
  │  • acks=all 설정으로 내구성 보장│
  └─────────────────────────────────┘
        │
        ▼
  STEP 7: Kafka Cluster
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  Topic: logs.app.events         │
  │  ┌─────┬─────┬─────┬─────┐      │
  │  │ P0  │ P1  │ P2  │ ... │      │  ← 12개 파티션
  │  └─────┴─────┴─────┴─────┘      │
  │  Replication Factor: 3          │
  │  Retention: 7 days              │
  └─────────────────────────────────┘
        │
        │ Consumer Group이 Pull
        ▼
  STEP 8: Spark Streaming / Flink Consumer
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  • 마이크로 배치 처리 (5분)      │
  │  • 파티션별 병렬 처리           │
  │  • 오프셋 관리                  │
  └─────────────────────────────────┘
        │
        ▼
  STEP 9: S3 Data Lake
  ─────────────────────────────
  ┌─────────────────────────────────┐
  │  s3://data-lake/raw/app-logs/   │
  │    └── dt=2024-01-15/           │
  │        └── hour=10/             │
  │            └── part-00001.parquet│
  └─────────────────────────────────┘

  ⏱️ 전체 소요 시간: 앱 → S3 약 5~10분 (준실시간)
```

---

## 9. 실무에서 자주 겪는 문제와 해결책

### 문제 1: 로그 유실

| 증상 | 원인 | 해결책 |
|------|------|--------|
| 앱에서 보낸 이벤트가 서버에 없음 | 네트워크 실패 후 재시도 없이 드롭 | 로컬 저장소에 저장 후 재시도 로직 구현 |
| Kafka에는 있지만 S3에 없음 | Consumer 오프셋 커밋 후 처리 실패 | `enable.auto.commit=false`로 수동 커밋 |
| 특정 시간대 데이터 누락 | ETL 작업 실패 | Airflow에서 재실행, 백필(Backfill) 처리 |

```python
# 재시도 로직 예시 (Python/Kotlin 공통 개념)
MAX_RETRIES = 3
BACKOFF_BASE = 2  # 지수 백오프 기본값

def send_with_retry(events, retry_count=0):
    try:
        response = http_client.post("/v1/events", events)
        if response.status_code == 202:
            return True
    except NetworkError:
        pass

    if retry_count < MAX_RETRIES:
        delay = BACKOFF_BASE ** retry_count + random.uniform(0, 1)  # Jitter 추가
        time.sleep(delay)
        return send_with_retry(events, retry_count + 1)
    else:
        save_to_local_storage(events)  # 최종 실패 시 로컬 저장
        return False
```

### 문제 2: 중복 이벤트

| 증상 | 원인 | 해결책 |
|------|------|--------|
| 동일 이벤트가 여러 번 기록됨 | 네트워크 타임아웃 후 재전송 | 클라이언트에서 UUID 기반 event_id 생성 |
| 집계 수치가 예상보다 높음 | Kafka Consumer 재처리 | Consumer에서 event_id 기반 중복 제거 |

```json
// 멱등성을 위한 event_id 포함
{
    "event_id": "550e8400-e29b-41d4-a716-446655440000",  // UUID v4
    "event_name": "purchase",
    "event_timestamp": "2024-01-15T10:30:00Z",
    "idempotency_key": "user123_purchase_SKU456_1705312200"  // 비즈니스 키
}
```

### 문제 3: 로그 지연

| 증상 | 원인 | 해결책 |
|------|------|--------|
| 실시간 대시보드에 데이터가 늦게 반영 | 배치 전송 주기가 너무 김 | 중요 이벤트는 즉시 전송, 나머지는 배치 |
| Kafka lag이 계속 증가 | Consumer 처리량 부족 | Consumer 인스턴스 추가, 파티션 증가 |
| S3 적재 지연 | Spark 작업 큐 대기 | EMR 클러스터 스케일 아웃 |

### 문제 4: 스키마 불일치

| 증상 | 원인 | 해결책 |
|------|------|--------|
| ETL 파싱 에러 발생 | 앱 버전별 스키마 차이 | Schema Registry 도입, 버전 필드 추가 |
| 새 필드가 null로 들어옴 | 구버전 앱에서 전송 | 기본값 설정, optional 필드로 처리 |

```json
// 스키마 버전 관리
{
    "schema_version": "2.1.0",
    "app_version": "3.2.1",
    "event_name": "purchase",
    "amount": 29900,
    "currency": "KRW",  // v2.0에서 추가된 필드
    "payment_method": "card"  // v2.1에서 추가된 필드
}
```

---

## 참고 자료

- [Kafka 공식 문서](https://kafka.apache.org/documentation/)
- [AWS Kinesis 개발자 가이드](https://docs.aws.amazon.com/kinesis/)
- [모바일 이벤트 로깅 베스트 프랙티스](https://segment.com/docs/connections/spec/)
- [Confluent Schema Registry](https://docs.confluent.io/platform/current/schema-registry/index.html)
- [Firebase Analytics 이벤트 설계 가이드](https://firebase.google.com/docs/analytics/events)
