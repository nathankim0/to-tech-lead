# Level 3: 데이터 파이프라인

## 학습 목표

모바일 앱에서 발생하는 데이터가 어떻게 수집되고, 가공되어, 최종적으로 분석 가능한 형태가 되는지 **전체 흐름을 이해**합니다.

- 로그 수집부터 데이터 웨어하우스까지의 End-to-End 파이프라인 이해
- 각 컴포넌트의 역할과 선택 기준 파악
- 실무에서 데이터 팀과 협업할 때 필요한 기본 지식 습득

---

## 소개: 데이터 파이프라인이란?

### 모바일 앱과 데이터의 여정

여러분이 만든 모바일 앱에서 사용자가 버튼을 탭하는 순간, 하나의 이벤트가 시작됩니다. 이 작은 이벤트는 긴 여정을 거쳐 "어제 DAU가 100만이었습니다"라는 숫자로 변환됩니다. 데이터 파이프라인은 이 여정을 가능하게 하는 시스템입니다.

```
사용자 탭 → 로그 생성 → 네트워크 전송 → 수집 서버 → 메시지 큐
    → 데이터 레이크 → ETL 처리 → 데이터 웨어하우스 → 대시보드
```

### 데이터 파이프라인의 핵심 특성

| 특성 | 설명 | 모바일 앱 예시 |
|------|------|----------------|
| **확장성 (Scalability)** | 데이터 양이 10배 늘어도 처리 가능 | 앱 출시 직후 사용자 폭증 대응 |
| **신뢰성 (Reliability)** | 데이터 유실 없이 안정적 전달 | 결제 이벤트는 절대 누락 불가 |
| **적시성 (Timeliness)** | 필요한 시점에 데이터 제공 | 실시간 푸시 vs 일간 리포트 |
| **비용 효율성** | 적절한 비용으로 운영 | 스타트업 예산에 맞는 설계 |

### 데이터 파이프라인의 구성 요소

```
┌─────────────────────────────────────────────────────────────────────────────────┐
│                        데이터 파이프라인 구성 요소                                   │
├─────────────────────────────────────────────────────────────────────────────────┤
│                                                                                 │
│  ┌───────────────────┐                                                          │
│  │  1. 데이터 소스    │  모바일 앱, 웹, 서버 로그, 외부 API                          │
│  │     (Sources)     │  → 데이터가 생성되는 모든 곳                                 │
│  └─────────┬─────────┘                                                          │
│            │                                                                    │
│            ▼                                                                    │
│  ┌───────────────────┐                                                          │
│  │  2. 수집 계층      │  Kafka, Kinesis, Fluentd                                  │
│  │   (Ingestion)     │  → 대용량 데이터를 안정적으로 수집                            │
│  └─────────┬─────────┘                                                          │
│            │                                                                    │
│            ▼                                                                    │
│  ┌───────────────────┐                                                          │
│  │  3. 저장 계층      │  S3, HDFS, GCS (Data Lake)                                │
│  │    (Storage)      │  → 원시 데이터를 저렴하게 무한 보관                           │
│  └─────────┬─────────┘                                                          │
│            │                                                                    │
│            ▼                                                                    │
│  ┌───────────────────┐                                                          │
│  │  4. 처리 계층      │  Spark, Flink, dbt, Airflow                               │
│  │  (Processing)     │  → 대용량 데이터를 변환하고 정제                              │
│  └─────────┬─────────┘                                                          │
│            │                                                                    │
│            ▼                                                                    │
│  ┌───────────────────┐                                                          │
│  │  5. 분석 계층      │  Data Warehouse, Trino, BI 도구                           │
│  │   (Analytics)     │  → 정제된 데이터로 인사이트 도출                              │
│  └───────────────────┘                                                          │
│                                                                                 │
└─────────────────────────────────────────────────────────────────────────────────┘
```

---

## 왜 모바일 개발자가 데이터 파이프라인을 알아야 하는가?

### 1. 로그 설계: 우리가 심는 로그가 데이터 파이프라인의 시작점

모바일 개발자가 작성하는 로그 코드가 데이터 파이프라인의 입력입니다. 잘못 설계된 로그는 아무리 좋은 파이프라인도 무용지물로 만듭니다.

```swift
// 나쁜 예: 분석이 불가능한 로그
Analytics.log("button_clicked")

// 좋은 예: 분석 가능한 구조화된 로그
Analytics.log(
    event: "purchase_completed",
    properties: [
        "product_id": "SKU123",
        "price": 29900,
        "currency": "KRW",
        "payment_method": "card",
        "session_id": "sess_abc123",
        "app_version": "3.2.1"
    ]
)
```

### 2. 디버깅: 이벤트가 누락되거나 지연될 때 어디서 문제인지 파악

PM: "어제 결제 이벤트가 대시보드에 안 나와요!"

이 상황에서 파이프라인을 이해하면:
- **로그 수집 단계**: 앱에서 이벤트가 전송되었는가? → 클라이언트 로그 확인
- **메시지 큐 단계**: Kafka에 메시지가 도착했는가? → Kafka lag 확인
- **ETL 단계**: Spark 작업이 실패했는가? → Airflow 로그 확인
- **웨어하우스 단계**: 데이터가 로드되었는가? → 테이블 최신 파티션 확인

### 3. 협업: 데이터 엔지니어, 분석가와 효과적으로 소통

```
[Before] 모바일 개발자 → 데이터 팀
"이벤트 이름 바꿨는데 대시보드 업데이트 해주세요"

[After] 모바일 개발자 → 데이터 팀
"v3.2에서 'purchase'를 'checkout_completed'로 변경했습니다.
- 하위 호환을 위해 v3.5까지는 두 이벤트 모두 발생합니다
- 새 이벤트에 'checkout_step' 필드가 추가되었습니다
- ETL에서 COALESCE로 처리하시거나, dbt 모델 업데이트가 필요합니다"
```

### 4. 테크리드 역량: 시스템 전체를 조망하는 관점 필요

테크리드는 클라이언트 코드만 보는 것이 아니라, 그 코드가 비즈니스에 미치는 영향까지 이해해야 합니다. 데이터 파이프라인 지식은 다음과 같은 의사결정에 필수입니다:

- 새로운 분석 기능 요청 시 구현 복잡도 판단
- 데이터 기반 기능(추천, 개인화) 설계
- 인프라 비용 최적화 논의 참여
- GDPR, 개인정보보호법 준수 설계

---

## 전체 파이프라인 개요

### 모바일 앱 데이터의 전체 여정

```
┌─────────────────────────────────────────────────────────────────────────────────────┐
│                         모바일 앱 데이터 파이프라인 전체 구조                            │
└─────────────────────────────────────────────────────────────────────────────────────┘

  [모바일 앱]                                                              [소비자]
  ┌─────────┐                                                           ┌──────────┐
  │  iOS    │                                                           │ 대시보드  │
  │ Android │                                                           │  (BI)    │
  │  Web    │                                                           └────▲─────┘
  └────┬────┘                                                                │
       │ HTTP/gRPC                                                           │
       │ (JSON Events)                                                       │
       ▼                                                                     │
┌──────────────┐                                                             │
│  수집 서버    │ ← Nginx, API Gateway                                        │
│ (Collector)  │                                                             │
└──────┬───────┘                                                             │
       │                                                                     │
       ▼                                                                     │
┌──────────────┐    ┌──────────────┐                                         │
│   메시지 큐   │    │   실시간      │                                         │
│   (Kafka)    │───▶│   소비자      │──────────────────────────────────┐      │
│              │    │ (Flink/KCL)  │                                   │      │
└──────┬───────┘    └──────────────┘                                   │      │
       │                   │                                           │      │
       │ 배치 (1시간)       │ 실시간 (초 단위)                             │      │
       ▼                   ▼                                           ▼      │
┌──────────────────────────────────────────────────────────────────────────────┐
│                              S3 DATA LAKE                                    │
├──────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│   Bronze Layer (Raw)        Silver Layer (Cleansed)    Gold Layer (Curated) │
│  ┌─────────────────┐       ┌─────────────────┐        ┌─────────────────┐   │
│  │ raw/events/     │  ───▶ │ processed/      │  ───▶  │ aggregated/     │   │
│  │   year=2024/    │ Spark │   events/       │  dbt   │   daily_metrics/│   │
│  │   month=01/     │       │   dt=2024-01-15/│        │   user_segments/│   │
│  │   day=15/       │       │                 │        │                 │   │
│  └─────────────────┘       └─────────────────┘        └─────────────────┘   │
│         │                          │                          │             │
│         └──────────────────────────┼──────────────────────────┘             │
│                                    │                                        │
└────────────────────────────────────┼────────────────────────────────────────┘
                                     │
                                     │ 데이터 로드
                                     ▼
                     ┌───────────────────────────────┐
                     │       DATA WAREHOUSE          │
                     │    (Redshift / BigQuery)      │
                     ├───────────────────────────────┤
                     │  ┌─────────┐   ┌─────────┐   │
                     │  │  Fact   │   │   Dim   │   │────────────▶ BI 도구
                     │  │ Tables  │   │ Tables  │   │    (Tableau, Looker)
                     │  └─────────┘   └─────────┘   │
                     │        Star Schema           │
                     └───────────────────────────────┘
                                     │
                     ┌───────────────┼───────────────┐
                     │               │               │
                     ▼               ▼               ▼
              ┌──────────┐   ┌──────────┐   ┌──────────┐
              │  Trino   │   │    ML    │   │  Alerts  │
              │ Ad-hoc   │   │ Pipeline │   │ 모니터링  │
              │  Query   │   │ (SageMaker)│  │ (Grafana)│
              └──────────┘   └──────────┘   └──────────┘
```

### 데이터 지연(Latency) 별 경로

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                        데이터 처리 경로 비교                                   │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. 실시간 경로 (Real-time Path) - 지연: 수 초                                │
│  ─────────────────────────────────────────────────────                      │
│     App → Kafka → Flink → Real-time Dashboard                               │
│     용도: 실시간 모니터링, 이상 탐지, 라이브 이벤트                              │
│                                                                             │
│  2. 준실시간 경로 (Near Real-time) - 지연: 수 분                               │
│  ─────────────────────────────────────────────────────                      │
│     App → Kafka → Spark Streaming → Data Lake → Trino                       │
│     용도: 최신 데이터 기반 분석 쿼리, 운영 대시보드                              │
│                                                                             │
│  3. 배치 경로 (Batch Path) - 지연: 수 시간                                    │
│  ─────────────────────────────────────────────────────                      │
│     App → Kafka → S3 (Raw) → Spark (ETL) → Warehouse → BI                   │
│     용도: 일간/주간 리포트, 정확한 집계, 대용량 분석                             │
│                                                                             │
│  4. 이력 분석 경로 (Historical Analysis) - 지연: 수 일                         │
│  ─────────────────────────────────────────────────────                      │
│     App → S3 (Archive) → Spark (Full Scan) → Ad-hoc Analysis                │
│     용도: 장기 트렌드 분석, A/B 테스트 결과, 코호트 분석                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 학습 내용

| 순서 | 주제 | 파일 | 핵심 질문 |
|------|------|------|-----------|
| 1 | 로그 수집 구조 | [01-log-collection.md](./01-log-collection.md) | 앱에서 보낸 로그는 어디로 가는가? |
| 2 | ETL | [02-etl.md](./02-etl.md) | 원시 데이터를 어떻게 쓸모 있게 만드는가? |
| 3 | S3 | [03-s3.md](./03-s3.md) | 왜 모든 데이터는 S3로 모이는가? |
| 4 | Spark | [04-spark.md](./04-spark.md) | 테라바이트 데이터를 어떻게 처리하는가? |
| 5 | Trino | [05-trino.md](./05-trino.md) | 분산된 데이터를 어떻게 한 번에 조회하는가? |
| 6 | Data Warehouse | [06-data-warehouse.md](./06-data-warehouse.md) | 분석용 데이터는 어떻게 구조화하는가? |

---

## 학습 순서와 의존성

### 권장 학습 경로

데이터의 흐름 순서대로 학습하는 것을 권장합니다. 각 문서는 이전 문서의 개념을 기반으로 합니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                           학습 로드맵                                         │
└─────────────────────────────────────────────────────────────────────────────┘

  ┌─────────────────────────────────────────────────────────────────────┐
  │                        PHASE 1: 데이터 입구                          │
  │                        (1-2일, 약 3시간)                             │
  │  ┌───────────────────────────────────────────────────────────────┐  │
  │  │  01. 로그 수집 구조                                            │  │
  │  │  ─────────────────                                            │  │
  │  │  - 모바일 앱에서 로그가 어떻게 전송되는지                         │  │
  │  │  - Kafka/Kinesis의 역할                                       │  │
  │  │  - 배치 vs 실시간 수집                                          │  │
  │  │                                                               │  │
  │  │  선수 지식: HTTP 통신, JSON 형식                                │  │
  │  │  학습 후: 로그 SDK 코드를 이해할 수 있음                          │  │
  │  └───────────────────────────────────────────────────────────────┘  │
  └─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │                       PHASE 2: 데이터 저장소                         │
  │                        (1-2일, 약 3시간)                             │
  │  ┌───────────────────────────────────────────────────────────────┐  │
  │  │  03. S3 (Data Lake)                                           │  │
  │  │  ──────────────────                                           │  │
  │  │  - 객체 스토리지의 개념                                          │  │
  │  │  - 파티셔닝과 파일 포맷                                          │  │
  │  │  - Bronze/Silver/Gold 계층 구조                                 │  │
  │  │                                                               │  │
  │  │  선수 지식: 01-로그 수집                                         │  │
  │  │  학습 후: 데이터가 S3에 어떻게 저장되는지 알 수 있음                │  │
  │  └───────────────────────────────────────────────────────────────┘  │
  └─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │                        PHASE 3: 데이터 변환                          │
  │                        (2-3일, 약 5시간)                             │
  │  ┌───────────────────────────────────────────────────────────────┐  │
  │  │  02. ETL + 04. Spark                                          │  │
  │  │  ─────────────────────                                        │  │
  │  │  - ETL vs ELT 개념                                            │  │
  │  │  - Spark의 분산 처리 원리                                       │  │
  │  │  - 대용량 데이터 변환 기법                                       │  │
  │  │                                                               │  │
  │  │  선수 지식: 01-로그 수집, 03-S3                                  │  │
  │  │  학습 후: Spark 코드를 읽고 의도를 파악할 수 있음                  │  │
  │  └───────────────────────────────────────────────────────────────┘  │
  └─────────────────────────────────────────────────────────────────────┘
                                    │
                                    ▼
  ┌─────────────────────────────────────────────────────────────────────┐
  │                        PHASE 4: 데이터 활용                          │
  │                        (2-3일, 약 4시간)                             │
  │  ┌───────────────────────────────────────────────────────────────┐  │
  │  │  05. Trino + 06. Data Warehouse                               │  │
  │  │  ─────────────────────────────────                            │  │
  │  │  - 분산 쿼리 엔진의 원리                                         │  │
  │  │  - 웨어하우스 모델링 (Star Schema)                               │  │
  │  │  - BI 도구와의 연동                                             │  │
  │  │                                                               │  │
  │  │  선수 지식: 모든 이전 문서, SQL 기본 문법                          │  │
  │  │  학습 후: 분석 쿼리를 작성하고 최적화할 수 있음                     │  │
  │  └───────────────────────────────────────────────────────────────┘  │
  └─────────────────────────────────────────────────────────────────────┘
```

### 문서 간 의존성 그래프

```
                    ┌────────────────────────────────────────┐
                    │         01-log-collection.md           │
                    │         (데이터 파이프라인의 입구)        │
                    └───────────────┬────────────────────────┘
                                    │
                                    ▼
                    ┌────────────────────────────────────────┐
                    │              03-s3.md                  │
                    │         (데이터가 저장되는 곳)           │
                    └───────────────┬────────────────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
        ┌───────────────────────┐     ┌───────────────────────┐
        │      02-etl.md        │     │     04-spark.md       │
        │   (변환의 개념과 설계)   │◀───▶│   (변환의 실행 엔진)    │
        └───────────┬───────────┘     └───────────┬───────────┘
                    │                             │
                    └───────────────┬─────────────┘
                                    │
                    ┌───────────────┴───────────────┐
                    │                               │
                    ▼                               ▼
        ┌───────────────────────┐     ┌───────────────────────┐
        │     05-trino.md       │     │ 06-data-warehouse.md  │
        │   (Ad-hoc 쿼리 엔진)   │◀───▶│   (분석용 데이터 저장)   │
        └───────────────────────┘     └───────────────────────┘
```

### 역할별 추천 학습 순서

#### 모바일 앱 개발자 (클라이언트)
```
01 → 03 → 06 (기본) → 나머지는 필요시
```
로그 설계와 데이터가 최종적으로 어떻게 사용되는지 이해하는 것이 중요합니다.

#### 백엔드 개발자
```
01 → 03 → 02 → 04 → 05 → 06 (전체)
```
데이터 파이프라인의 서버 사이드 구현을 이해해야 합니다.

#### 테크리드 / 아키텍트
```
전체 순서대로 학습 + 각 문서의 "실무 시나리오" 섹션 집중
```
시스템 전체 설계와 트레이드오프를 이해해야 합니다.

---

## 핵심 용어 사전

| 용어 | 설명 | 관련 문서 |
|------|------|-----------|
| **Event** | 사용자 행동을 나타내는 데이터 단위 (클릭, 구매 등) | 01 |
| **Kafka** | 분산 메시지 큐, 대용량 이벤트 수집에 사용 | 01 |
| **Data Lake** | 원시 데이터를 저장하는 저장소 (주로 S3) | 03 |
| **ETL** | Extract-Transform-Load, 데이터 변환 프로세스 | 02 |
| **Spark** | 대용량 분산 데이터 처리 엔진 | 04 |
| **Trino** | 분산 SQL 쿼리 엔진 (Presto 포크) | 05 |
| **Data Warehouse** | 분석용으로 구조화된 데이터 저장소 | 06 |
| **Partition** | 데이터를 날짜 등으로 분할하여 저장 | 03, 04 |
| **Star Schema** | 중심 Fact 테이블과 Dimension 테이블 구조 | 06 |
| **DAU/MAU** | Daily/Monthly Active Users, 핵심 지표 | 전체 |

---

## AI 학습 프롬프트 예시

아래 프롬프트를 Claude나 ChatGPT에 입력하여 심화 학습하세요.

### 데이터 엔지니어 역할 프롬프트

```
당신은 10년 경력의 시니어 데이터 엔지니어입니다.
모바일 앱 개발자인 저에게 데이터 파이프라인을 설명해주세요.

저희 앱은 일일 활성 사용자(DAU) 100만 명이고,
사용자당 평균 50개의 이벤트를 발생시킵니다.

다음 질문에 답해주세요:
1. 이 규모에서 어떤 로그 수집 아키텍처를 추천하시나요?
2. Kafka와 Kinesis 중 어떤 것을 선택해야 할까요?
3. ETL 파이프라인은 어떻게 설계해야 할까요?
4. 데이터 지연(latency)은 어느 정도가 적정한가요?

실제 사례와 트레이드오프를 포함해서 설명해주세요.
```

### 트러블슈팅 시나리오 프롬프트

```
시나리오: 모바일 앱에서 전송한 이벤트 로그가 분석 대시보드에 나타나지 않습니다.

다음 파이프라인 단계별로 확인해야 할 사항을 알려주세요:
1. 로그 수집 단계 (Kafka/Kinesis)
2. 데이터 레이크 단계 (S3)
3. ETL 처리 단계 (Spark)
4. 데이터 웨어하우스 단계 (Redshift)

각 단계에서 사용할 수 있는 디버깅 명령어나 모니터링 지표도 알려주세요.
```

### 아키텍처 설계 프롬프트

```
시나리오: 신규 모바일 게임을 출시합니다. 예상 규모는:
- 출시 첫 주 DAU: 10만
- 3개월 후 예상 DAU: 500만
- 이벤트: 게임 시작, 스테이지 클리어, 아이템 구매, 광고 시청

다음을 설계해주세요:
1. 초기 MVP 파이프라인 (비용 최소화)
2. 스케일업 시 전환 계획
3. 각 단계별 기술 스택 선택과 이유
4. 예상 월 비용 (AWS 기준)

스타트업 예산을 고려하여 현실적인 제안을 해주세요.
```

---

## 산출물

이 레벨을 완료하면 다음 산출물을 작성할 수 있어야 합니다.

### 1. 데이터 파이프라인 다이어그램
- 자신의 서비스에 맞는 파이프라인 구조도
- 각 컴포넌트 선택 이유 명시
- 예상 데이터 볼륨과 지연 시간 표기

### 2. ETL 설계 문서
- 원시 로그 스키마 정의
- 변환 로직 명세
- 파티셔닝 전략
- 에러 처리 방안

### 3. Trino 쿼리 정리 노트
- 자주 사용하는 분석 쿼리 템플릿
- 성능 최적화 팁
- 다른 데이터 소스 연동 방법

---

## 실무 체크리스트

학습 완료 후 아래 항목을 체크해보세요:

### 기본 이해
- [ ] 내가 작성한 로그가 대시보드에 나타나기까지의 경로를 설명할 수 있다
- [ ] Kafka, S3, Spark, Data Warehouse의 역할을 구분할 수 있다
- [ ] ETL과 ELT의 차이를 설명할 수 있다
- [ ] 배치 처리와 실시간 처리의 트레이드오프를 이해한다

### 실무 적용
- [ ] 로그 이벤트 스키마를 설계할 때 분석 요구사항을 고려할 수 있다
- [ ] 데이터 누락 시 어느 단계에서 문제인지 추론할 수 있다
- [ ] 데이터 팀과 협업할 때 적절한 용어로 소통할 수 있다
- [ ] 새로운 분석 요청에 대해 파이프라인 변경 범위를 추정할 수 있다

### 심화
- [ ] 간단한 Spark 코드를 읽고 의도를 파악할 수 있다
- [ ] Trino로 분석 쿼리를 작성할 수 있다
- [ ] Star Schema 기반 테이블 설계를 이해할 수 있다
- [ ] GDPR 등 개인정보 처리 요구사항을 파이프라인에 반영할 수 있다

---

## 선수 지식

- Level 1: 서버 기초 (HTTP, REST API)
- Level 2: 서버 인프라 기초 (클라우드, 컨테이너)
- SQL 기본 문법

---

## 예상 학습 시간

| 항목 | 시간 | 비고 |
|------|------|------|
| 문서 읽기 | 4-5시간 | 6개 문서, 각 40-50분 |
| AI 프롬프트 실습 | 2-3시간 | 심화 질문과 시나리오 |
| 산출물 작성 | 4-6시간 | 자기 서비스 기준 설계 |
| **총 예상 시간** | **10-15시간** | 1-2주 분량 |

---

## 다음 단계

Level 4에서는 데이터 활용 관점에서 **AI/ML 시스템**을 다룹니다.

- ML 모델 학습 파이프라인
- Feature Store
- 실시간 추론 시스템
- A/B 테스트와 실험 플랫폼

---

## 참고 자료

### 책
- "Designing Data-Intensive Applications" - Martin Kleppmann
- "The Data Warehouse Toolkit" - Ralph Kimball
- "Fundamentals of Data Engineering" - Joe Reis, Matt Housley

### 온라인 자료
- AWS Big Data Blog
- Netflix Tech Blog (데이터 파이프라인 관련 글)
- Uber Engineering Blog
- Airbnb Engineering Blog

### 도구 공식 문서
- Apache Kafka: https://kafka.apache.org/documentation/
- Apache Spark: https://spark.apache.org/docs/latest/
- Trino: https://trino.io/docs/current/
- dbt: https://docs.getdbt.com/
